{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d061e33a-51ba-4ff7-a03b-af8f19411488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Incremental Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7388795-737c-4236-88df-089f1e3d1cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mentioning clouddiles as csv + # we need make a heirarchical folders to have data in the bronze layer and also maintaining a checkpoint which will basiccally maintain the files and contain info of the files which are processed and which are not processed. Also checkpoint should be in its own as its easier in production\n",
    "df = spark.readStream.format(\"cloudFiles\")\\\n",
    "        .option(\"cloudFiles.format\", \"csv\")\\\n",
    "        .option(\"cloudFiles.schemaLocation\",\"/Volumes/workspace/bronze/bronze_volume/bookings/checkpoint\")\\\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\\\n",
    "        .load(\"/Volumes/workspace/raw_schema/rawdata_volume/rawdata/bookings/\")\n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73284cb7-3de5-4869-8e2d-9c0201ef53fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing the action, # trigger()once --- to automatically turn off the cluster once it is done, so only up and running when needed \n",
    "df.writeStream.format(\"delta\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/workspace/bronze/bronze_volume/bookings/checkpoint\")\\\n",
    "    .option(\"path\", \"/Volumes/workspace/bronze/bronze_volume/bookings/data\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de7bb911-0b07-462a-be32-bdec8c42684c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
